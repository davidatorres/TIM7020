{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6 - Institution to Congress Database creation\n",
    "\n",
    "[GitHub Repo Link](https://github.com/davidatorres/TIM7020/tree/main/Week6)\n",
    "\n",
    "### Perform normal library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import mysql.connector as mysql\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "host=os.getenv('MYSQL_SERVER')\n",
    "database=os.getenv('MYSQL_DATABASE')\n",
    "user=os.getenv('MYSQL_USER')\n",
    "password=os.getenv('MYSQL_PASSWORD')\n",
    "\n",
    "import internal_functions as fn\n",
    "\n",
    "IMPUTATIONVALUES = \"'A','B','C','D','G','H','J','K','L','N','P','R','Y','Z',''\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove and Create the generated /sql directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"sql\"\n",
    "\n",
    "# Remove the directory if it exists\n",
    "if os.path.exists(directory):\n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "# Create the directory\n",
    "os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROP & CREATE the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = fn.create_dbconnection()\n",
    "dropDBQuery = \"DROP SCHEMA IF EXISTS tim7020;\"\n",
    "createDBQuery = \"CREATE SCHEMA tim7020;\"\n",
    "fn.execute_dbquery(query=dropDBQuery, db=db, cursor=cursor)\n",
    "fn.execute_dbquery(query=createDBQuery, db=db, cursor=cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create intutitive table names using OpenAI API and the column's description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "### Process the IPEDS data tables and create new column names using OpenAI API\n",
    "###\n",
    "\n",
    "### Read the dictionary files for the database\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TableName, dbDictionary.TIM7020TableName))\n",
    "### Get a list of the tables that have already been processed\n",
    "processedTables = fn.return_list_of_processed_tables()\n",
    "###\n",
    "### For each table, generate the new variable names using OpenAI API\n",
    "### Write the new variable names to a \".new.\" dictionary file, \n",
    "###  so we're not having to make repeated calls to the API\n",
    "###\n",
    "for oldTableName, newTableName in tableNames:\n",
    "    if oldTableName.upper() not in processedTables:\n",
    "        ### Record the table we are working on\n",
    "        ##print(f'oldTableName: {oldTableName}, newTableName: {newTableName}')\n",
    "        ### Read table's dictionary file\n",
    "        tableDictionary = pd.read_excel(f'dictionary/{oldTableName}.xlsx', sheet_name=1)\n",
    "        # Create list of column descriptions to be used as input to the API\n",
    "        varibleTitles = tableDictionary.iloc[:,6].tolist()\n",
    "        # Print the number of variables in the list of column names\n",
    "        print(f'varibleTitles (length): {len(varibleTitles)}')\n",
    "        # Group the column names into groups of 30 to make the API calls\n",
    "        groups = [varibleTitles[i:i+20] for i in range(0, len(varibleTitles), 20)]\n",
    "        varibleTitlesNew = []\n",
    "        for group in groups:\n",
    "            varibleTitlesNew.extend(fn.create_name_from_description(group))\n",
    "            time.sleep(5)\n",
    "        # Print the number of variables in the list of new column names\n",
    "        print(f'varibleTitlesNew (length): {len(varibleTitlesNew)}')\n",
    "        # Append the new column names to the table's dictionary file\n",
    "        tableDictionary['varname_new'] = varibleTitlesNew\n",
    "        # Write the new dictionary file to disk\n",
    "        tableDictionary.to_excel(f'dictionary/{oldTableName.lower()}.new.xlsx', sheet_name='varlist', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a cross reference map between old and new tables and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oldTable</th>\n",
       "      <th>oldVarname</th>\n",
       "      <th>newTable</th>\n",
       "      <th>newVarname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HD2021</td>\n",
       "      <td>UNITID</td>\n",
       "      <td>institution</td>\n",
       "      <td>inst_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HD2021</td>\n",
       "      <td>INSTNM</td>\n",
       "      <td>institution</td>\n",
       "      <td>inst_name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HD2021</td>\n",
       "      <td>IALIAS</td>\n",
       "      <td>institution</td>\n",
       "      <td>inst_alias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HD2021</td>\n",
       "      <td>ADDR</td>\n",
       "      <td>institution</td>\n",
       "      <td>address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HD2021</td>\n",
       "      <td>CITY</td>\n",
       "      <td>institution</td>\n",
       "      <td>city</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  oldTable oldVarname     newTable  newVarname\n",
       "0   HD2021     UNITID  institution     inst_id\n",
       "1   HD2021     INSTNM  institution   inst_name\n",
       "2   HD2021     IALIAS  institution  inst_alias\n",
       "3   HD2021       ADDR  institution     address\n",
       "4   HD2021       CITY  institution        city"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "### Read the dictionary file for the database\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbVarTable = pd.read_excel('@dictionary.xlsx', sheet_name='vartable21')\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TableName, dbDictionary.TIM7020TableName))\n",
    "### \n",
    "### For each table, read the dictionary file and build the Old Column Name to New Column Name Map \n",
    "###\n",
    "tableMap = pd.DataFrame()\n",
    "for oldTableName, newTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    #print(f'oldTableName: {oldTableName}, newTableName: {newTableName}')\n",
    "    ### Read table's dictionary file\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{oldTableName}.new.xlsx', sheet_name='varlist')\n",
    "    ### Create a dictionary of the old table and old column names to new table name and new column names\n",
    "    for _, row in tableDictionary.iterrows():\n",
    "        tableMap = tableMap.append({\n",
    "            'oldTable' : oldTableName, \n",
    "            'oldVarname' : row.varname,\n",
    "            'newTable' : newTableName,\n",
    "            'newVarname' : row.varname_new\n",
    "        }, ignore_index=True)\n",
    "\n",
    "tableMap.to_excel('@mapTableColumn.xlsx', index=False)\n",
    "tableMap.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CREATE and ALTER TABLE queries for the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = fn.create_dbconnection()\n",
    "###\n",
    "### Read the dictionary file for the database\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbRefValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "loopCount = 0\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TableName, dbDictionary.TIM7020TableName))\n",
    "\n",
    "### \n",
    "### For each table, read the dictionary file and build the CREATE TABLE/ALTER TABLE queries \n",
    "### to build the IPEDS Postsecondary database using the IPEDS dictionary and data files\n",
    "### The novel opportunity here is to use the dictory file to determine the data type of the columns\n",
    "###   create new human readable column names and added the column description as column comments \n",
    "###\n",
    "for oldTableName, newTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    #print(f'oldTableName: {oldTableName}, newTableName: {newTableName}')\n",
    "    ### Read table's dictionary file\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{oldTableName}.new.xlsx', sheet_name='varlist')\n",
    "    # Create list of column descriptions less the first row (IPEDS ID)\n",
    "    variableNames = tableDictionary['varname_new'].tolist()\n",
    "    ### Read first 100 rows of the data file to verify data types\n",
    "    tableData = pd.read_csv(f'data/{oldTableName}.csv', nrows=100, encoding=\"utf-8\", na_values=['.', '. ', ' '])\n",
    "    ### Create string for the query\n",
    "    createTableQuery = None\n",
    "    alterTableQuery = f'ALTER TABLE {newTableName} \\n'\n",
    "    ###\n",
    "    ### Loop through the rows of the table dictionary file\n",
    "    ###\n",
    "    for index, row in tableDictionary.iterrows():\n",
    "        ### Setup the column creation variables\n",
    "        addColumn = None\n",
    "        addImputed = None\n",
    "        addCheck = None\n",
    "        dataType = None\n",
    "        ### Skip the first row (inst_id), we'll create the table with this common column\n",
    "        if index == 0:\n",
    "            createTableQuery = f'CREATE TABLE {newTableName} ({row.varname_new} INTEGER);'\n",
    "            continue\n",
    "\n",
    "        ### If there is an imputation variable, add the column to the table\n",
    "        if (isinstance(row.imputationvar, str) == True) and (row.imputationvar != 'None' and row.imputationvar[0].upper() == 'X'):\n",
    "            ###\n",
    "            ### Create the ADD COLUMN line for the imputed flag column to be added to the ALTER TABLE query\n",
    "            ### \n",
    "            addImputed = f'  ADD COLUMN {row.varname_new}_imp ENUM({IMPUTATIONVALUES}) COMMENT \"{row.varname}|{row.varTitle}\",'\n",
    "        ###\n",
    "        ### Determine the data type to use for the field based on the metadata\n",
    "        ###\n",
    "        dtype = ''\n",
    "        if tableData[row.varname.upper()].dtype == 'float':\n",
    "            dtype = 'float'\n",
    "        dataType = fn.dtype_by_format(row.DataType, dtype, row.Fieldwidth)\n",
    "        ###\n",
    "        ### Create the ADD COLUMN line to be added to the ALTER TABLE query\n",
    "        ###    \n",
    "        addColumn = f'    ADD COLUMN {row.varname_new} {dataType} COMMENT \"{row.varname}|{row.varTitle}\",'\n",
    "        ###\n",
    "        ### Build the ALTER TABLE query for the current column\n",
    "        ###\n",
    "        addToQuery = ''\n",
    "        if addImputed != None:\n",
    "            addToQuery = addImputed + '\\n'\n",
    "        if addColumn != None:\n",
    "            addToQuery += addColumn + '\\n'\n",
    "        alterTableQuery += addToQuery\n",
    "\n",
    "        ### Continue for the next row in the table dictionary file\n",
    "        continue\n",
    "    ###\n",
    "    ### Remove the last comma and space, add a semicolon\n",
    "    ###\n",
    "    alterTableQuery = alterTableQuery[:-2] + ';'\n",
    "\n",
    "    ###\n",
    "    ### Write the DROP, CREATE, ALTER TABLE queries to a SQL script file\n",
    "    ###\n",
    "    dropTableQuery = f'DROP TABLE IF EXISTS {newTableName};'\n",
    "    fn.write_sql_file(f'{newTableName}.1.drop table', dropTableQuery)\n",
    "    fn.write_sql_file(f'{newTableName}.2.create table', createTableQuery)\n",
    "    fn.write_sql_file(f'{newTableName}.3.add columns', alterTableQuery)\n",
    "\n",
    "    ###\n",
    "    ### Execute the DROP, CREATE, and ALTER the table SQL queries\n",
    "    ###\n",
    "    fn.execute_dbquery(query=dropTableQuery, db=db, cursor=cursor)\n",
    "    fn.execute_dbquery(query=createTableQuery, db=db, cursor=cursor)\n",
    "    fn.execute_dbquery(query=alterTableQuery, db=db, cursor=cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload CSV files into database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = fn.create_dbconnection()\n",
    "engine = fn.create_dbengine()\n",
    "###\n",
    "### Read the dictionary file for the databasea\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbRefValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "loopCount = 0\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TableName, dbDictionary.TIM7020TableName))\n",
    "\n",
    "### \n",
    "### For each table, read the dictionary file and build the CREATE TABLE/ALTER TABLE queries \n",
    "### to build the IPEDS Postsecondary database using the IPEDS dictionary and data files\n",
    "### The novel opportunity here is to use the dictory file to determine the data type of the columns\n",
    "###   create new human readable column names and added the column description as column comments \n",
    "###\n",
    "for oldTableName, newTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    #print(f'oldTableName: {oldTableName}, newTableName: {newTableName}')\n",
    "    ### Read table's dictionary file\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{oldTableName}.new.xlsx', sheet_name='varlist')\n",
    "    ### Create a dictionary of old column names and new column names\n",
    "    columnNames = dict(zip(tableDictionary['varname'], tableDictionary['varname_new']))\n",
    "    impColumnNames = tableDictionary[tableDictionary.imputationvar.str.len() > 2][['imputationvar', 'varname_new']]\n",
    "    impColumnNames.varname_new = impColumnNames.varname_new + '_imp'\n",
    "    impColumnNames = dict(zip(impColumnNames['imputationvar'], impColumnNames['varname_new']))\n",
    "    tableData = pd.read_csv(f'data/{oldTableName.lower()}.csv', encoding='latin1', na_values=['.', '. ', ' '])\n",
    "    ### Rename the columns  \n",
    "    tableData = tableData.rename(columns=columnNames)\n",
    "    tableData = tableData.rename(columns=impColumnNames)\n",
    "    ### Write data to database\n",
    "    tableData.to_sql(name=newTableName, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set PK constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = fn.create_dbconnection()\n",
    "engine = fn.create_dbengine()\n",
    "###\n",
    "### Read the dictionary file for the databasea\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbRefValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "loopCount = 0\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TableName, dbDictionary.TIM7020TableName))\n",
    "\n",
    "### \n",
    "### For each table, read the dictionary file and build the CREATE TABLE/ALTER TABLE queries \n",
    "### to build the IPEDS Postsecondary database using the IPEDS dictionary and data files\n",
    "### The novel opportunity here is to use the dictory file to determine the data type of the columns\n",
    "###   create new human readable column names and added the column description as column comments \n",
    "###\n",
    "for oldTableName, newTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    #print(f'oldTableName: {oldTableName}, newTableName: {newTableName}')\n",
    "    ### Read table's dictionary file\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{oldTableName}.new.xlsx', sheet_name='varlist')\n",
    "    ### Create a list of columns with a PK designation\n",
    "    pkColumns = list(tableDictionary[tableDictionary['imputationvar'] == 'PK']['varname_new'])\n",
    "    addPKQuery = f'ALTER TABLE {newTableName}\\n    ADD PRIMARY KEY ({\", \".join(pkColumns)});'\n",
    "\n",
    "    ###\n",
    "    ### Write the ALTER TABLE query to a SQL script file\n",
    "    ###\n",
    "    fn.write_sql_file(f'{newTableName}.4.add PK', addPKQuery)\n",
    "\n",
    "    ###\n",
    "    ### Execute the ALTER TABLE SQL query\n",
    "    ###\n",
    "    fn.execute_dbquery(query=addPKQuery, db=db, cursor=cursor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reference tables for discrete table columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = fn.create_dbconnection()\n",
    "engine = fn.create_dbengine()\n",
    "###\n",
    "### Read the dictionary file for the databasea\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbRefValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "loopCount = 0\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TableName, dbDictionary.TIM7020TableName))\n",
    "\n",
    "### \n",
    "### For each table, read the dictionary file and build the CREATE TABLE/ALTER TABLE queries \n",
    "### to build the IPEDS Postsecondary database using the IPEDS dictionary and data files\n",
    "### The novel opportunity here is to use the dictory file to determine the data type of the columns\n",
    "###   create new human readable column names and added the column description as column comments \n",
    "###\n",
    "for oldTableName, newTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    #print(f'oldTableName: {oldTableName}, newTableName: {newTableName}')\n",
    "\n",
    "    ### IF olTableName is equal to IC2021_CAMPUSES, continue to the next table (reusing reference values)\n",
    "    if oldTableName == 'IC2021_CAMPUSES':\n",
    "        continue\n",
    "\n",
    "    ### Read the reference values for the table\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{oldTableName.lower()}.new.xlsx', sheet_name='varlist')\n",
    "    tableData = pd.read_csv(f'data/{oldTableName.lower()}.csv', nrows=100, encoding=\"utf-8\", na_values=['.', '. ', ' '])\n",
    "    ### Filter tableDictionary to only the rows format is equal to Disc, select only varname and varname_new columns\n",
    "    tableDictionary = tableDictionary[tableDictionary.format == 'Disc'].sort_values(by=['varname'])\n",
    "\n",
    "    allRefValues = dbRefValues[dbRefValues.TableName == oldTableName]\n",
    "    allRefValues = allRefValues.sort_values(by=['varName', 'valueOrder'])\n",
    "\n",
    "    for _, row in tableDictionary.iterrows():\n",
    "        # Print which colomn are we working on\n",
    "        # print(f'{row.DataType}, {row.Fieldwidth}, {row.varname}, {row.varname_new}')\n",
    "\n",
    "        # filter allRefValues to only the rows where varName is equal to the current row varname\n",
    "        refValues = allRefValues[allRefValues.varName == row.varname]\n",
    "        ## if refValues is empty, continue to the next row\n",
    "        if refValues.empty:\n",
    "            # #print(f'oldTableName: {oldTableName}, newTableName: {newTableName}')\n",
    "            print(f'No refValues for {row.varname}, {row.varname_new}')\n",
    "            continue\n",
    "        \n",
    "        ### Create DROP TABLE query\n",
    "        dropRefTableQuery = f'DROP TABLE IF EXISTS institution_xref_{row.varname_new};'\n",
    "        ### CREATE CREATE TABLE query\n",
    "        # Determine the data type of the column\n",
    "        dtype = ''\n",
    "        if tableData[row.varname.upper()].dtype == 'float':\n",
    "            dtype = 'float'\n",
    "        dataType = fn.dtype_by_format(row.DataType, dtype, row.Fieldwidth)\n",
    "        # Build the CREATE TABLE query text\n",
    "        createRefTableQuery = f'''\n",
    "            CREATE TABLE institution_xref_{row.varname_new} (\n",
    "                Codevalue {dataType},\n",
    "                valueLabel VARCHAR(255),\n",
    "                valueOrder MEDIUMINT UNSIGNED,\n",
    "                PRIMARY KEY (Codevalue)\n",
    "            );'''\n",
    "        ###\n",
    "        ### Write the DROP, CREATE, ALTER TABLE queries to a SQL script file\n",
    "        ###\n",
    "        fn.write_sql_file(f'institution_xref_{row.varname_new}.1.drop table', dropRefTableQuery)\n",
    "        fn.write_sql_file(f'institution_xref_{row.varname_new}.2.create table', createRefTableQuery)\n",
    "        ###\n",
    "        ### Execute the DROP, CREATE and INSERT queries\n",
    "        ###\n",
    "        fn.execute_dbquery(query=dropRefTableQuery, db=db, cursor=cursor)\n",
    "        fn.execute_dbquery(query=createRefTableQuery, db=db, cursor=cursor)\n",
    "        refValues = refValues[['Codevalue', 'valueLabel', 'valueOrder']]\n",
    "        refValues.to_sql(name=f'institution_xref_{row.varname_new}', con=engine, if_exists='append', index=False)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure that campus fips county codes and cbsa codes are in xref tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = fn.create_dbconnection()\n",
    "\n",
    "insertQuery = '''\n",
    "    insert into tim7020.institution_xref_fips_county_code (CodeValue, valueLabel)\n",
    "        select distinct fips_county_code, county_name\n",
    "            from tim7020.institution_campus \n",
    "            where fips_county_code not in (\n",
    "                select Codevalue from tim7020.institution_xref_fips_county_code\n",
    "            );'''\n",
    "fn.execute_dbquery(query=insertQuery, db=db, cursor=cursor)\n",
    "\n",
    "insertQuery = '''\n",
    "    insert into tim7020.institution_xref_cbsa (Codevalue, valueLabel)\n",
    "        select distinct cbsa as Codevalue, concat(city, ', ', state) as valueLabel\n",
    "            from tim7020.institution_campus \n",
    "            where cbsa not in (\n",
    "                select Codevalue from tim7020.institution_xref_cbsa\n",
    "                );'''\n",
    "fn.execute_dbquery(query=insertQuery, db=db, cursor=cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FK and CHECK contraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = fn.create_dbconnection()\n",
    "engine = fn.create_dbengine()\n",
    "###\n",
    "### Read the dictionary file for the databasea\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbRefValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "###\n",
    "### Create a list of child tables to apply foreign key constraints\n",
    "###\n",
    "### dbDictionary = dbDictionary[dbDictionary.TIM7020TableName != 'institution']\n",
    "\n",
    "loopCount = 0\n",
    "\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TableName, dbDictionary.TIM7020TableName))\n",
    "### \n",
    "### For each table, read the dictionary file and build the ALTER TABLE queries \n",
    "###     to create the foreign keys constraints to the 'institution' parent table or\n",
    "###     the 'institution_xref_{column name}' reference table\n",
    "###\n",
    "for oldTableName, newTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    #print(f'oldTableName: {oldTableName}, newTableName: {newTableName}')\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{oldTableName.lower()}.new.xlsx', sheet_name='varlist')\n",
    "    tableDictionary = tableDictionary[tableDictionary.format == 'Disc']\n",
    "\n",
    "    alterTableQuery = ''\n",
    "\n",
    "    if newTableName != 'institution':\n",
    "        alterTableQuery = f'''\n",
    "            ALTER TABLE {newTableName} \n",
    "                ADD FOREIGN KEY (inst_id) REFERENCES institution(inst_id)'''\n",
    "        \n",
    "    for _, row in tableDictionary.iterrows():\n",
    "        varname = row.varname\n",
    "        if oldTableName == 'IC2021_CAMPUSES':\n",
    "            varname = tableMap[tableMap.oldVarname == row.varname].oldVarname.values[0][2:]\n",
    "\n",
    "        ### Filter dbRefValues for the current variable\n",
    "        refValues = dbRefValues[dbRefValues.varName == varname]\n",
    "\n",
    "        if len(alterTableQuery) != 0:\n",
    "            alterTableQuery += f''',\n",
    "                '''\n",
    "        else:\n",
    "            alterTableQuery += f'''ALTER TABLE {newTableName}\n",
    "                '''\n",
    "            \n",
    "        if row['DataType'] == 'A' or len(refValues) > 7:\n",
    "            alterTableQuery += f'ADD FOREIGN KEY ({row.varname_new}) REFERENCES institution_xref_{row.varname_new}(Codevalue)'\n",
    "        else:\n",
    "            alterTableQuery += f'ADD CHECK ({row.varname_new} IN ({\", \".join(refValues.Codevalue.to_list())}))'\n",
    "\n",
    "    alterTableQuery += ';'\n",
    "    ###\n",
    "    ### Write the ALTER Table FK query to a file\n",
    "    ###\n",
    "    fn.write_sql_file(f'{newTableName}.5.add FK-Check', alterTableQuery)\n",
    "    ###\n",
    "    ### Execute the ALTER Table FK query\n",
    "    ###\n",
    "    fn.execute_dbquery(query=alterTableQuery, db=db, cursor=cursor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Congress Tables and Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6286"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "### Create congressional tables in Week6 database\n",
    "###\n",
    "db, cursor = fn.create_dbconnection()\n",
    "engine = fn.create_dbengine()\n",
    "###\n",
    "### Create tables using the exported SQL file @congress.sql\n",
    "###  file was exported from MySQL Workbench\n",
    "###  slight modifications were made to the file \n",
    "###  for the new institution primary key (inst_id)\n",
    "###  and elimination of the us_states_territories table\n",
    "###  and assigning the state column in the congress table to\n",
    "###  the Codevalue column institution_xref_state table\n",
    "###\n",
    "with open('@congress.sql', 'r') as file:\n",
    "    query = file.read()\n",
    "###\n",
    "### split into separate queries\n",
    "###\n",
    "query = query.replace(';', ';|')\n",
    "query = query.replace('| ', '|')\n",
    "query = query.replace('\\n', '  ')\n",
    "queries = query.split('|')\n",
    "###\n",
    "### execute each query\n",
    "###\n",
    "for query in queries:\n",
    "    query = query.strip()\n",
    "    query = query.replace('  ', ' ')\n",
    "    query = query.replace('(  ', '(')\n",
    "    query = query.replace(' )', ')')\n",
    "    fn.execute_dbquery(query=query, db=db, cursor=cursor)\n",
    "\n",
    "###\n",
    "### Copy WEEK 5 congress data, save to Excel, and the tim7020 database\n",
    "###\n",
    "\n",
    "# df = pd.read_sql('SELECT * FROM mydatabase.chamber', con=db)\n",
    "# df.to_excel('data/congress_chamber.xlsx', sheet_name='chamber', index=False)\n",
    "df = pd.read_excel('data/congress_chamber.xlsx', sheet_name='chamber')\n",
    "df.to_sql('congress_chamber', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# df = pd.read_sql('SELECT * FROM mydatabase.party', con=db)\n",
    "# df.to_excel('data/congress_party.xlsx', sheet_name='party', index=False)\n",
    "df = pd.read_excel('data/congress_party.xlsx', sheet_name='party')\n",
    "df.to_sql('congress_party', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# df = pd.read_sql('SELECT * FROM mydatabase.congress', con=db)\n",
    "# df.to_excel('data/congress.xlsx', sheet_name='congress', index=False)\n",
    "df = pd.read_excel('data/congress.xlsx', sheet_name='congress')\n",
    "df.to_sql('congress', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# df = pd.read_sql('SELECT * FROM mydatabase.congress_office', con=db)\n",
    "# df.to_excel('data/congress_office.xlsx', sheet_name='congress_office', index=False)\n",
    "df = pd.read_excel('data/congress_office.xlsx', sheet_name='congress_office')\n",
    "df.to_sql('congress_office', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# df = pd.read_sql('SELECT * FROM mydatabase.institution_to_congress', con=db)\n",
    "### Rename column to inst_id in Week 6  \n",
    "# df.rename(columns={'ipeds_id': 'inst_id'}, inplace=True)\n",
    "# df.to_excel('data/institution_to_congress.xlsx', sheet_name='institution_to_congress', index=False)\n",
    "df = pd.read_excel('data/institution_to_congress.xlsx', sheet_name='institution_to_congress')\n",
    "df.to_sql('institution_to_congress', con=engine, if_exists='append', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
