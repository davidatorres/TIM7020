{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import mysql.connector as mysql\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "host=os.getenv('MYSQL_SERVER')\n",
    "database=os.getenv('MYSQL_DATABASE')\n",
    "user=os.getenv('MYSQL_USER')\n",
    "password=os.getenv('MYSQL_PASSWORD')\n",
    "\n",
    "IMPUTATIONVALUES = \"'A','B','C','D','G','H','J','K','L','N','P','R','Z',''\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used within this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtype_by_format(format, dtype, width):\n",
    "    dataType = None\n",
    "    ###\n",
    "    ### If the column is an alpha column, set the data type to VARCHAR for the specified field width\n",
    "    ###\n",
    "    if format[0] == 'A':\n",
    "        dataType = f'VARCHAR({width})'\n",
    "\n",
    "    ###\n",
    "    ### If the column is not-alpha, determine the numeric data type and field width\n",
    "    ###\n",
    "    else:\n",
    "        # Check to see if data type is integer or float, but looking at how the data would import\n",
    "        if dtype == 'float':\n",
    "            if width > 8:\n",
    "                dataType = 'DOUBLE'\n",
    "            else:\n",
    "                dataType = 'FLOAT'\n",
    "        else:\n",
    "            if width > 8:\n",
    "                dataType = 'BIGINT'\n",
    "            elif width > 6:\n",
    "                dataType = 'INT'\n",
    "            elif width in [5, 6]:\n",
    "                dataType = 'MEDIUMINT'\n",
    "            elif width in [3, 4]:\n",
    "                dataType = 'SMALLINT'\n",
    "            elif width in [1, 2]:\n",
    "                dataType = 'TINYINT'\n",
    "            else:\n",
    "                dataType = 'INT'\n",
    "    ###\n",
    "    ### If the data type is still None, set it to TEXT\n",
    "    ###\n",
    "    if dataType == None:\n",
    "         print(f'**WARNING**: {row.varname_new} has no data type, setting to TEXT')\n",
    "         dataType = 'TEXT'\n",
    "    ###\n",
    "    ### Return the data type\n",
    "    ###\n",
    "    return dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtype_by_datatype(dtype, length):\n",
    "    if dtype == 'int64':\n",
    "        return 'INTEGER'\n",
    "    elif dtype == 'float64':\n",
    "        return 'REAL'\n",
    "    elif dtype == 'object':\n",
    "        return 'TEXT'\n",
    "    else:\n",
    "        return 'TEXT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sql_file(sql_file, sql_query):\n",
    "    with open(f'sql/{sql_file}.sql', 'w') as f:\n",
    "        f.write(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dbengine():\n",
    "    ### Create SQLAlchmey engine\n",
    "    # Create the engine to connect to the MySQL database\n",
    "    connect_args={'ssl':{'fake_flag_to_enable_tls': True}}\n",
    "    return create_engine(f'mysql+pymysql://{user}:{password}@{host}:3306/{database}', connect_args=connect_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dbconnection():\n",
    "    try:\n",
    "        db = mysql.connect(host=host, user=user, password=password, database=database)\n",
    "        return db, db.cursor()\n",
    "    except mysql.Error as e:\n",
    "        if e.errno == mysql.ER_ACCESS_DENIED_ERROR:\n",
    "            print(\"Something is wrong with your user name or password\")\n",
    "            return None\n",
    "        elif e.errno == mysql.ER_BAD_DB_ERROR:\n",
    "            print(\"Database does not exist\")\n",
    "            return None\n",
    "        else:\n",
    "            print(e)\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_dbquery(query, db=None, cursor=None):\n",
    "    # If no db or cursor is provided, connect to the database\n",
    "    if db is None or cursor is None:\n",
    "        db, cursor = create_dbconnection()\n",
    "\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        db.commit()\n",
    "        # time.sleep(5)\n",
    "    except mysql.Error as e:\n",
    "        print(f\"Failed creating database with query: {query} - Error: {e}\")\n",
    "        exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Create column names from the column descriptions using GPT-3.5-turbo OpenAI API\n",
    "###\n",
    "def create_name_from_description(titles):\n",
    "    titleList = \"\"\n",
    "    for title in titles:\n",
    "        title = title.replace(\",\", \"\").replace(\"'\", \"\").replace(\".\", \"\").lower()\n",
    "        titleList += f'[{title}],'\n",
    "    message = {\"role\": \"user\", \"content\": f\"{titleList}\"}\n",
    "    messages = [\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "            Produce a list of readable database column names using the list of Column Descriptions provided.\n",
    "            Column Names should NOT start with a number, be MySQL reserved word or Python keyword. \n",
    "            All special characters are to be replaced with an underscore and the Column Name should be all lowercase. \n",
    "            Make sure Column Names are readable, descriptive, concise, consistent, unique, and less than 30 characters including underscores.\n",
    "            Return one entry for each Column Name submitted by the user.\n",
    "            DO NOT return a numbered list.\n",
    "            Substitute common words in the Column Descriptions with abbreviations such as:\n",
    "                'inst' replaces 'institution'\n",
    "                'id' replaces 'identification'\n",
    "                'class' replaces 'classification'\n",
    "                'cd' replaces 'code'\n",
    "                'org' replaces 'organization'\n",
    "                'loc' replaces 'location'\n",
    "                'cat' replaces 'category'\n",
    "                'url' replaces 'web address'\n",
    "                'url' replaces 'website address'\n",
    "                'rpt' replaces 'report'\n",
    "            Returned Column Name(s) should be in square brackets.\"\"\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"[Unique identification number of the institution]\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"[uid]\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"[Institution (entity) name], [Institution name alias]\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"[inst_name], [inst_alias]\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"[Title of chief administrator], \n",
    "                [Disability Services Web Address], \n",
    "                [Institution's internet web address], \n",
    "                [Office of Postsecondary Education (OPE) ID Number], \n",
    "                [Sector of institution]\"\"\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"[title_chief_administrator], [disability_url], [inst_url], [ope_id], [sector]\"\n",
    "        }\n",
    "    ]\n",
    "    messages.append(message)\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response.choices[0]['message']['content'].replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").split(\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Get list of dictionary files already processed through OpenAI API\n",
    "###   - This is to avoid re-processing the same files again\n",
    "###\n",
    "def return_list_of_processed_tables():\n",
    "    directory_path = 'dictionary/'\n",
    "    file_pattern = '*.new.xlsx'\n",
    "\n",
    "    files_list = glob.glob(directory_path + '/' + file_pattern)\n",
    "    new_files_list = []\n",
    "    for each_file in files_list:\n",
    "        new_files_list.append(each_file.replace(directory_path, '').replace('.new.xlsx', '').upper())\n",
    "\n",
    "    return new_files_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new table names using OpenAI API using the column description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "### Process the IPEDS data tables and create new column names using OpenAI API\n",
    "###\n",
    "\n",
    "### Read the dictionary files for the database\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TIM7020TableName, dbDictionary.TableName))\n",
    "### Get a list of the tables that have already been processed\n",
    "processedTables = return_list_of_processed_tables()\n",
    "###\n",
    "### For each table, generate the new variable names using OpenAI API\n",
    "### Write the new variable names to a \".new.\" dictionary file, \n",
    "###  so we're not having to make repeated calls to the API\n",
    "###\n",
    "for newTableName, ipedsTableName in tableNames:\n",
    "    if ipedsTableName.upper() not in processedTables:\n",
    "        ### Record the table we are working on\n",
    "        print(f'newTableName: {newTableName}, ipedsTableName: {ipedsTableName}')\n",
    "        ### Read table's dictionary file\n",
    "        tableDictionary = pd.read_excel(f'dictionary/{ipedsTableName}.xlsx', sheet_name=1)\n",
    "        # Create list of column descriptions to be used as input to the API\n",
    "        varibleTitles = tableDictionary.iloc[:,6].tolist()\n",
    "        # Print the number of variables in the list of column names\n",
    "        print(f'varibleTitles (length): {len(varibleTitles)}')\n",
    "        # Group the column names into groups of 30 to make the API calls\n",
    "        groups = [varibleTitles[i:i+20] for i in range(0, len(varibleTitles), 20)]\n",
    "        varibleTitlesNew = []\n",
    "        for group in groups:\n",
    "            varibleTitlesNew.extend(create_name_from_description(group))\n",
    "            time.sleep(5)\n",
    "        # Print the number of variables in the list of new column names\n",
    "        print(f'varibleTitlesNew (length): {len(varibleTitlesNew)}')\n",
    "        # Append the new column names to the table's dictionary file\n",
    "        tableDictionary['varname_new'] = varibleTitlesNew\n",
    "        # Write the new dictionary file to disk\n",
    "        tableDictionary.to_excel(f'dictionary/{ipedsTableName.lower()}.new.xlsx', sheet_name='varlist', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CREATE and ALTER TABLE queries for the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = create_dbconnection()\n",
    "###\n",
    "### Read the dictionary file for the database\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbRefValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "loopCount = 0\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TIM7020TableName, dbDictionary.TableName))\n",
    "\n",
    "### \n",
    "### For each table, read the dictionary file and build the CREATE TABLE/ALTER TABLE queries \n",
    "### to build the IPEDS Postsecondary database using the IPEDS dictionary and data files\n",
    "### The novel opportunity here is to use the dictory file to determine the data type of the columns\n",
    "###   create new human readable column names and added the column description as column comments \n",
    "###\n",
    "for newTableName, ipedsTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    print(f'newTableName: {newTableName}, ipedsTableName: {ipedsTableName}')\n",
    "    ### Read table's dictionary file\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{ipedsTableName}.new.xlsx', sheet_name='varlist')\n",
    "    # Create list of column descriptions less the first row (IPEDS ID)\n",
    "    variableNames = tableDictionary['varname_new'].tolist()\n",
    "    ### Read first 100 rows of the data file to verify data types\n",
    "    tableData = pd.read_csv(f'data/{ipedsTableName}.csv', nrows=100, encoding=\"utf-8\", na_values=['.', ' '])\n",
    "    ### Create string for the query\n",
    "    createTableQuery = None\n",
    "    alterTableQuery = f'ALTER TABLE {newTableName} \\n'\n",
    "    ###\n",
    "    ### Loop through the rows of the table dictionary file\n",
    "    ###\n",
    "    for index, row in tableDictionary.iterrows():\n",
    "        ### Setup the column creation variables\n",
    "        addColumn = None\n",
    "        addImputed = None\n",
    "        addCheck = None\n",
    "        dataType = None\n",
    "        ### Skip the first row (inst_id), we'll create the table with this common column\n",
    "        if index == 0:\n",
    "            createTableQuery = f'CREATE TABLE {newTableName} ({row.varname_new} INTEGER);'\n",
    "            continue\n",
    "\n",
    "        ### If there is an imputation variable, add the column to the table\n",
    "        if (isinstance(row.imputationvar, str) == True) and (row.imputationvar != 'None' and row.imputationvar[0].upper() == 'X'):\n",
    "            ###\n",
    "            ### Create the ADD COLUMN line for the imputed flag column to be added to the ALTER TABLE query\n",
    "            ### \n",
    "            addImputed = f'  ADD COLUMN {row.varname_new}_imp ENUM({IMPUTATIONVALUES}) COMMENT \"{row.varname}|{row.varTitle}\",'\n",
    "        ###\n",
    "        ### Determine the data type to use for the field based on the metadata\n",
    "        ###\n",
    "        dtype = ''\n",
    "        if tableData[row.varname.upper()].dtype == 'float':\n",
    "            dtype = 'float'\n",
    "        dataType = dtype_by_format(row.format, dtype, row.Fieldwidth)\n",
    "        ###\n",
    "        ### Create the ADD COLUMN line to be added to the ALTER TABLE query\n",
    "        ###    \n",
    "        addColumn = f'    ADD COLUMN {row.varname_new} {dataType} COMMENT \"{row.varname}|{row.varTitle}\",'\n",
    "        ###\n",
    "        ### Build the ALTER TABLE query for the current column\n",
    "        ###\n",
    "        addToQuery = ''\n",
    "        if addImputed != None:\n",
    "            addToQuery = addImputed + '\\n'\n",
    "        if addColumn != None:\n",
    "            addToQuery += addColumn + '\\n'\n",
    "        alterTableQuery += addToQuery\n",
    "\n",
    "        ### Continue for the next row in the table dictionary file\n",
    "        continue\n",
    "    ###\n",
    "    ### Remove the last comma and space, add a semicolon\n",
    "    ###\n",
    "    alterTableQuery = alterTableQuery[:-2] + ';'\n",
    "\n",
    "    ###\n",
    "    ### Write the DROP, CREATE, ALTER TABLE queries to a SQL script file\n",
    "    ###\n",
    "    dropTableQuery = f'DROP TABLE IF EXISTS {newTableName};'\n",
    "    write_sql_file(f'{newTableName}.1.drop table', dropTableQuery)\n",
    "    write_sql_file(f'{newTableName}.2.create table', createTableQuery)\n",
    "    write_sql_file(f'{newTableName}.3.add columns', alterTableQuery)\n",
    "\n",
    "    ###\n",
    "    ### Execute the DROP, CREATE, and ALTER the table SQL queries\n",
    "    ###\n",
    "    execute_dbquery(query=dropTableQuery, db=db, cursor=cursor)\n",
    "    execute_dbquery(query=createTableQuery, db=db, cursor=cursor)\n",
    "    execute_dbquery(query=alterTableQuery, db=db, cursor=cursor)\n",
    "\n",
    "    loopCount += 1\n",
    "    if loopCount > 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload CSV files into database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = create_dbconnection()\n",
    "engine = create_dbengine()\n",
    "###\n",
    "### Read the dictionary file for the databasea\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbRefValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "loopCount = 0\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TIM7020TableName, dbDictionary.TableName))\n",
    "\n",
    "### \n",
    "### For each table, read the dictionary file and build the CREATE TABLE/ALTER TABLE queries \n",
    "### to build the IPEDS Postsecondary database using the IPEDS dictionary and data files\n",
    "### The novel opportunity here is to use the dictory file to determine the data type of the columns\n",
    "###   create new human readable column names and added the column description as column comments \n",
    "###\n",
    "for newTableName, ipedsTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    print(f'newTableName: {newTableName}, ipedsTableName: {ipedsTableName}')\n",
    "    ### Read table's dictionary file\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{ipedsTableName}.new.xlsx', sheet_name='varlist')\n",
    "    ### Create a dictionary of old column names and new column names\n",
    "    columnNames = dict(zip(tableDictionary['varname'], tableDictionary['varname_new']))\n",
    "    ### Read in the CSV data file\n",
    "    tableData = pd.read_csv(f'data/{ipedsTableName.lower()}.csv', encoding='latin-1')\n",
    "    ### Rename the columns  \n",
    "    tableData = tableData.rename(columns=columnNames)\n",
    "    ### Write data to database\n",
    "    tableData.to_sql(name=newTableName, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "    loopCount += 1\n",
    "    if loopCount > 2:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set PK constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = create_dbconnection()\n",
    "engine = create_dbengine()\n",
    "###\n",
    "### Read the dictionary file for the databasea\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbRefValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "loopCount = 0\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TIM7020TableName, dbDictionary.TableName))\n",
    "\n",
    "### \n",
    "### For each table, read the dictionary file and build the CREATE TABLE/ALTER TABLE queries \n",
    "### to build the IPEDS Postsecondary database using the IPEDS dictionary and data files\n",
    "### The novel opportunity here is to use the dictory file to determine the data type of the columns\n",
    "###   create new human readable column names and added the column description as column comments \n",
    "###\n",
    "for newTableName, ipedsTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    print(f'newTableName: {newTableName}, ipedsTableName: {ipedsTableName}')\n",
    "    ### Read table's dictionary file\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{ipedsTableName}.new.xlsx', sheet_name='varlist')\n",
    "    ### Create a list of columns with a PK designation\n",
    "    pkColumns = list(tableDictionary[tableDictionary['imputationvar'] == 'PK']['varname_new'])\n",
    "    addPKQuery = f'ALTER TABLE {newTableName}\\n    ADD PRIMARY KEY ({\", \".join(pkColumns)});'\n",
    "\n",
    "    ###\n",
    "    ### Write the ALTER TABLE query to a SQL script file\n",
    "    ###\n",
    "    write_sql_file(f'{newTableName}.4.add PK', addPKQuery)\n",
    "\n",
    "    ###\n",
    "    ### Execute the ALTER TABLE SQL query\n",
    "    ###\n",
    "    execute_dbquery(query=addPKQuery, db=db, cursor=cursor)\n",
    "\n",
    "    loopCount += 1\n",
    "    if loopCount > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reference tables for discrete table columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newTableName: institution, ipedsTableName: HD2021\n",
      "newTableName: institution_ic_response_flags, ipedsTableName: FLAGS2021\n",
      "newTableName: institution_ic_offerings, ipedsTableName: IC2021\n"
     ]
    }
   ],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = create_dbconnection()\n",
    "engine = create_dbengine()\n",
    "###\n",
    "### Read the dictionary file for the databasea\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "dbRefValues = pd.read_excel('@dictionary.xlsx', sheet_name='valuesets21')\n",
    "loopCount = 0\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TIM7020TableName, dbDictionary.TableName))\n",
    "\n",
    "### \n",
    "### For each table, read the dictionary file and build the CREATE TABLE/ALTER TABLE queries \n",
    "### to build the IPEDS Postsecondary database using the IPEDS dictionary and data files\n",
    "### The novel opportunity here is to use the dictory file to determine the data type of the columns\n",
    "###   create new human readable column names and added the column description as column comments \n",
    "###\n",
    "for newTableName, ipedsTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    print(f'newTableName: {newTableName}, ipedsTableName: {ipedsTableName}')\n",
    "    ### Skip the IC2021_CAMPUSES table, as it uses reference tables from other tables\n",
    "    if ipedsTableName == 'IC2021_CAMPUSES':\n",
    "        continue\n",
    "\n",
    "    ### Read the reference values for the table\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{ipedsTableName.lower()}.new.xlsx', sheet_name='varlist')\n",
    "    tableData = pd.read_csv(f'data/{ipedsTableName.lower()}.csv', nrows=100, encoding=\"utf-8\", na_values=['.', ' '])\n",
    "    ### Filter tableDictionary to only the rows format is equal to Disc, select only varname and varname_new columns\n",
    "    tableDictionary = tableDictionary[tableDictionary.format == 'Disc'].sort_values(by=['varname'])\n",
    "\n",
    "    allRefValues = dbRefValues[dbRefValues.TableName == ipedsTableName]\n",
    "    allRefValues = allRefValues.sort_values(by=['varName', 'valueOrder'])\n",
    "\n",
    "    for _, row in tableDictionary.iterrows():\n",
    "        # Print which colomn are we working on\n",
    "        # print(f'{row.DataType}, {row.Fieldwidth}, {row.varname}, {row.varname_new}')\n",
    "\n",
    "        # filter allRefValues to only the rows where varName is equal to the current row varname\n",
    "        refValues = allRefValues[allRefValues.varName == row.varname]\n",
    "        ## if refValues is empty, continue to the next row\n",
    "        if refValues.empty:\n",
    "            # print(f'newTableName: {newTableName}, ipedsTableName: {ipedsTableName}')\n",
    "            print(f'No refValues for {row.varname}, {row.varname_new}')\n",
    "            continue\n",
    "        \n",
    "        ### Create DROP TABLE query\n",
    "        dropRefTableQuery = f'DROP TABLE IF EXISTS institution_xref_{row.varname_new};'\n",
    "        ### CREATE CREATE TABLE query\n",
    "        # Determine the data type of the column\n",
    "        dtype = ''\n",
    "        if tableData[row.varname.upper()].dtype == 'float':\n",
    "            dtype = 'float'\n",
    "        dataType = dtype_by_format(row.format, dtype, row.Fieldwidth)\n",
    "        # Build the CREATE TABLE query text\n",
    "        createRefTableQuery = f'''\n",
    "            CREATE TABLE institution_xref_{row.varname_new} (\n",
    "                Codevalue {dataType},\n",
    "                valueLabel VARCHAR(255),\n",
    "                valueOrder SMALLINT UNSIGNED,\n",
    "                PRIMARY KEY (Codevalue)\n",
    "            );'''\n",
    "        ###\n",
    "        ### Write the DROP, CREATE, ALTER TABLE queries to a SQL script file\n",
    "        ###\n",
    "        write_sql_file(f'institution_xref_{row.varname_new}.1.drop table', dropTableQuery)\n",
    "        write_sql_file(f'institution_xref_{row.varname_new}.2.create table', createTableQuery)\n",
    "        ###\n",
    "        ### Execute the DROP, CREATE and INSERT queries\n",
    "        ###\n",
    "        execute_dbquery(query=dropRefTableQuery, db=db, cursor=cursor)\n",
    "        execute_dbquery(query=createRefTableQuery, db=db, cursor=cursor)\n",
    "        refValues = refValues[['Codevalue', 'valueLabel', 'valueOrder']]\n",
    "        refValues.to_sql(name=f'institution_xref_{row.varname_new}', con=engine, if_exists='replace', index=False)   \n",
    "\n",
    "    loopCount += 1\n",
    "    if loopCount > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FK contraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newTableName: institution_ic_response_flags, ipedsTableName: FLAGS2021\n",
      "newTableName: institution_ic_offerings, ipedsTableName: IC2021\n",
      "newTableName: institution_ic_academic_charges, ipedsTableName: IC2021_AY\n"
     ]
    }
   ],
   "source": [
    "### Connect to database and create cursor\n",
    "db, cursor = create_dbconnection()\n",
    "engine = create_dbengine()\n",
    "###\n",
    "### Read the dictionary file for the databasea\n",
    "###\n",
    "dbDictionary = pd.read_excel('@dictionary.xlsx', sheet_name='Tables21')\n",
    "###\n",
    "### Create a list of child tables to apply foreign key constraints\n",
    "###\n",
    "dbDictionary = dbDictionary[dbDictionary.TIM7020TableName != 'institution']\n",
    "\n",
    "loopCount = 0\n",
    "\n",
    "### Create a list of tuples with the table names and the data files\n",
    "tableNames = list(zip(dbDictionary.TIM7020TableName, dbDictionary.TableName))\n",
    "### \n",
    "### For each table, read the dictionary file and build the ALTER TABLE queries \n",
    "###     to create the foreign keys constraints to the 'institution' parent table or\n",
    "###     the 'institution_xref_{column name}' reference table\n",
    "###\n",
    "for newTableName, ipedsTableName in tableNames:\n",
    "    ### Record the table we are working on\n",
    "    print(f'newTableName: {newTableName}, ipedsTableName: {ipedsTableName}')\n",
    "    tableDictionary = pd.read_excel(f'dictionary/{ipedsTableName.lower()}.new.xlsx', sheet_name='varlist')\n",
    "\n",
    "    alterTableQuery = f'''\n",
    "        ALTER TABLE {newTableName} \n",
    "        ADD CONSTRAINT fk_{newTableName}_institution_instid\n",
    "            FOREIGN KEY (instid) REFERENCES institution(instid)\n",
    "    '''\n",
    "\n",
    "    loopCount += 1\n",
    "    if loopCount > 2:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSSenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
